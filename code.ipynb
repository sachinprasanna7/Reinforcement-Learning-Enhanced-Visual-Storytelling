{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, CLIPProcessor, CLIPModel, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Pdogg Windows10\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# Load CLIP and GPT-2 models\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\") \n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clip_model.to(device)\n",
    "gpt2_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption_with_clip_gpt2(image_path):\n",
    "    # Load and process image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get CLIP image embeddings\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "\n",
    "    # Normalize image features and prepare input for GPT-2\n",
    "    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)\n",
    "    image_features_text = image_features.detach().cpu().numpy().astype(str)\n",
    "\n",
    "    # Use GPT-2 to generate a caption from the image features\n",
    "    input_ids = gpt2_tokenizer.encode(\"Image caption: \", return_tensors=\"pt\").to(device)\n",
    "    outputs = gpt2_model.generate(input_ids, max_length=50, do_sample=True)\n",
    "\n",
    "    caption = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_folder = 'Dataset/Organized_Annotations/'\n",
    "image_folder = 'Dataset/SSID_Images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []\n",
    "for i in range(1, 11):\n",
    "    image_path = f\"{image_folder}/{i}.jpg\"\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/SSID_Images//1.jpg: Image caption:  Rationalist - he has no understanding of the concept of money\n",
      "\n",
      "In an article in the Mirror, Mr Smith writes: \"I'm really confused because I thought this would help people understand the issue, if there was\n",
      "Dataset/SSID_Images//2.jpg: Image caption:  A new research study suggests that the way our bodies work may need more research\n",
      "\n",
      "The study is published in the European Journal of Nursing Science paper, the third published in two weeks.\n",
      "\n",
      "Dr Susan Eger, an\n",
      "Dataset/SSID_Images//3.jpg: Image caption:  Anastasia de Valta: \"If you take a look at the map we have, you will start to see where the island should lie and where we should go\".\n",
      "\n",
      "In March 2011, in an online\n",
      "Dataset/SSID_Images//4.jpg: Image caption:  It will run at 15mph in 4.5sec\n",
      "\n",
      "\n",
      "Brief news - The BBC's Jonathan Head has had the first ever view of Anderton's new new electric car. It's like flying into a hurricane\n",
      "Dataset/SSID_Images//5.jpg: Image caption:  A view from the bottom of the lake\n",
      "\n",
      "Image caption:  A view to the bottom of the lake\n",
      "\n",
      "Image caption:  A view to the top of river\n",
      "\n",
      "Image caption:  A\n",
      "Dataset/SSID_Images//6.jpg: Image caption:  Lorraine H. Nesbitt said the school was being run at the expense of the rest of the district: \"That's really the kind of policy they're running away from, as well as, from me\n",
      "Dataset/SSID_Images//7.jpg: Image caption:  The city has been hit by several storms for nearly a week\n",
      "\n",
      "Image copyright Getty Images Image caption Residents are being urged to stay indoors\n",
      "\n",
      "Image copyright AFP Image caption Police have been deployed to protect pedestrians and businesses\n",
      "\n",
      "\n",
      "Dataset/SSID_Images//8.jpg: Image caption:  \"No, it was just me,\" wrote Michael Stowell, then a teenager, with a group of friends across Cambridge, \"after this was over.\"\n",
      "\n",
      "Their \"good times\" were to last up to 70 years\n",
      "Dataset/SSID_Images//9.jpg: Image caption:  The 'Lethal Breath' was made into an international anthem\n",
      "\n",
      "At least three of the protesters are from South Kashmir and were arrested and questioned for some six hours after arriving at the park, but the incident may come\n",
      "Dataset/SSID_Images//10.jpg: Image caption:  The number of people using an address in the UK is still relatively small – but some people are using it much more than others. We asked about what is a legitimate government-issued ID and what is not.\n",
      "\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "captions = {image_path: generate_caption_with_clip_gpt2(image_path) for image_path in image_paths}\n",
    "\n",
    "for img, caption in captions.items():\n",
    "    print(f\"{img}: {caption}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def generate_caption(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\generation\\utils.py:1258: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "image_captions = {image_path: generate_caption(image_path) for image_path in image_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/SSID_Images//1.jpg: a group of people walking up a snowy slope\n",
      "Dataset/SSID_Images//2.jpg: a person on a snowboard on a mountain\n",
      "Dataset/SSID_Images//3.jpg: a man climbing up a snowy mountain\n",
      "Dataset/SSID_Images//4.jpg: a man standing on top of a mountain\n",
      "Dataset/SSID_Images//5.jpg: a man sitting on top of a snowy mountain\n",
      "Dataset/SSID_Images//6.jpg: a man climbing up a mountain with a helmet on\n",
      "Dataset/SSID_Images//7.jpg: a field with a fence and mountains in the background\n",
      "Dataset/SSID_Images//8.jpg: a man with a backpack on a trail\n",
      "Dataset/SSID_Images//9.jpg: the summit of the mountain is covered in snow\n",
      "Dataset/SSID_Images//10.jpg: a man wearing a blue shirt\n"
     ]
    }
   ],
   "source": [
    "for img, caption in image_captions.items():\n",
    "    print(f\"{img}: {caption}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organising the Image-Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Dataset/Organized_Annotations/SSID_Train_Organized.json') as f:\n",
    "    organized_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_data = {}\n",
    "\n",
    "album_limit = 200\n",
    "processed_album_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for album_id in organized_data.items():\n",
    "    if processed_album_count >= album_limit:\n",
    "        break \n",
    "\n",
    "    album_id = album_id[1]\n",
    "\n",
    "    # get the value of the first key in the dictionary\n",
    "    stories = album_id[list(album_id.keys())[0]]\n",
    "\n",
    "    for item in stories:\n",
    "\n",
    "        # add a key value pair to the dictionary, key being the image_id and value being the storytext\n",
    "        fine_tune_data[item['image_id']] = item['storytext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625\n"
     ]
    }
   ],
   "source": [
    "print(len(fine_tune_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(fine_tune_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_tune_data = dict(sorted(fine_tune_data.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "#print(sorted_fine_tune_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, data_dict, image_folder, processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dict (dict): A dictionary where keys are image names, and values are captions.\n",
    "            image_folder (str): Path to the folder containing the images.\n",
    "            processor (BlipProcessor): The processor to preprocess the images and captions.\n",
    "        \"\"\"\n",
    "        self.data_dict = data_dict\n",
    "        self.image_folder = image_folder\n",
    "        self.processor = processor\n",
    "        self.image_keys = list(data_dict.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_keys)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_key = self.image_keys[idx]\n",
    "        caption = self.data_dict[image_key]\n",
    "        \n",
    "        image_path = f\"{self.image_folder}/{image_key}.jpg\"  \n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        inputs = self.processor(images=image, text=caption, return_tensors=\"pt\", padding=\"max_length\", truncation=True)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": inputs['input_ids'].squeeze(),  \n",
    "            \"attention_mask\": inputs['attention_mask'].squeeze(),  \n",
    "            \"pixel_values\": inputs['pixel_values'].squeeze()  \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_fine_tune_data = dict(list(sorted_fine_tune_data.items())[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataset/SSID_Images//1.jpg': 'a group of people walking up a snowy slope', 'Dataset/SSID_Images//2.jpg': 'a person on a snowboard on a mountain', 'Dataset/SSID_Images//3.jpg': 'a man climbing up a snowy mountain', 'Dataset/SSID_Images//4.jpg': 'a man standing on top of a mountain', 'Dataset/SSID_Images//5.jpg': 'a man sitting on top of a snowy mountain', 'Dataset/SSID_Images//6.jpg': 'a man climbing up a mountain with a helmet on', 'Dataset/SSID_Images//7.jpg': 'a field with a fence and mountains in the background', 'Dataset/SSID_Images//8.jpg': 'a man with a backpack on a trail', 'Dataset/SSID_Images//9.jpg': 'the summit of the mountain is covered in snow', 'Dataset/SSID_Images//10.jpg': 'a man wearing a blue shirt'}\n"
     ]
    }
   ],
   "source": [
    "print(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': 'a group of people walking up a snowy slope', '2': 'a person on a snowboard on a mountain', '3': 'a man climbing up a snowy mountain', '4': 'a man standing on top of a mountain', '5': 'a man sitting on top of a snowy mountain', '6': 'a man climbing up a mountain with a helmet on', '7': 'a field with a fence and mountains in the background', '8': 'a man with a backpack on a trail', '9': 'the summit of the mountain is covered in snow', '10': 'a man wearing a blue shirt'}\n"
     ]
    }
   ],
   "source": [
    "image_captions = {k.split('/')[-1].split('.')[0]: v for k, v in image_captions.items()}\n",
    "\n",
    "print(image_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pdogg Windows10\\Desktop\\Semester 7\\Computer Vision\\Reinforcement-Learning-Enhanced-Visual-Storytelling\\cv\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "Loss: 13.018089294433594\n",
      "Epoch 2/3\n",
      "Loss: 11.234161376953125\n",
      "Epoch 3/3\n",
      "Loss: 9.930630683898926\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "learning_rate = 5e-5\n",
    "batch_size = 10\n",
    "\n",
    "dataset = ImageCaptionDataset(image_captions, image_folder, processor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    \n",
    "    for batch in data_loader:\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        pixel_values = batch['pixel_values'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "image_paths = []\n",
    "\n",
    "for i in range(11, 111):\n",
    "    image_path = f\"{image_folder}/{i}.jpg\"\n",
    "    image_paths.append(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset/SSID_Images//11.jpg', 'Dataset/SSID_Images//12.jpg', 'Dataset/SSID_Images//13.jpg', 'Dataset/SSID_Images//14.jpg', 'Dataset/SSID_Images//15.jpg', 'Dataset/SSID_Images//16.jpg', 'Dataset/SSID_Images//17.jpg', 'Dataset/SSID_Images//18.jpg', 'Dataset/SSID_Images//19.jpg', 'Dataset/SSID_Images//20.jpg', 'Dataset/SSID_Images//21.jpg', 'Dataset/SSID_Images//22.jpg', 'Dataset/SSID_Images//23.jpg', 'Dataset/SSID_Images//24.jpg', 'Dataset/SSID_Images//25.jpg', 'Dataset/SSID_Images//26.jpg', 'Dataset/SSID_Images//27.jpg', 'Dataset/SSID_Images//28.jpg', 'Dataset/SSID_Images//29.jpg', 'Dataset/SSID_Images//30.jpg', 'Dataset/SSID_Images//31.jpg', 'Dataset/SSID_Images//32.jpg', 'Dataset/SSID_Images//33.jpg', 'Dataset/SSID_Images//34.jpg', 'Dataset/SSID_Images//35.jpg', 'Dataset/SSID_Images//36.jpg', 'Dataset/SSID_Images//37.jpg', 'Dataset/SSID_Images//38.jpg', 'Dataset/SSID_Images//39.jpg', 'Dataset/SSID_Images//40.jpg', 'Dataset/SSID_Images//41.jpg', 'Dataset/SSID_Images//42.jpg', 'Dataset/SSID_Images//43.jpg', 'Dataset/SSID_Images//44.jpg', 'Dataset/SSID_Images//45.jpg', 'Dataset/SSID_Images//46.jpg', 'Dataset/SSID_Images//47.jpg', 'Dataset/SSID_Images//48.jpg', 'Dataset/SSID_Images//49.jpg', 'Dataset/SSID_Images//50.jpg', 'Dataset/SSID_Images//51.jpg', 'Dataset/SSID_Images//52.jpg', 'Dataset/SSID_Images//53.jpg', 'Dataset/SSID_Images//54.jpg', 'Dataset/SSID_Images//55.jpg', 'Dataset/SSID_Images//56.jpg', 'Dataset/SSID_Images//57.jpg', 'Dataset/SSID_Images//58.jpg', 'Dataset/SSID_Images//59.jpg', 'Dataset/SSID_Images//60.jpg', 'Dataset/SSID_Images//61.jpg', 'Dataset/SSID_Images//62.jpg', 'Dataset/SSID_Images//63.jpg', 'Dataset/SSID_Images//64.jpg', 'Dataset/SSID_Images//65.jpg', 'Dataset/SSID_Images//66.jpg', 'Dataset/SSID_Images//67.jpg', 'Dataset/SSID_Images//68.jpg', 'Dataset/SSID_Images//69.jpg', 'Dataset/SSID_Images//70.jpg', 'Dataset/SSID_Images//71.jpg', 'Dataset/SSID_Images//72.jpg', 'Dataset/SSID_Images//73.jpg', 'Dataset/SSID_Images//74.jpg', 'Dataset/SSID_Images//75.jpg', 'Dataset/SSID_Images//76.jpg', 'Dataset/SSID_Images//77.jpg', 'Dataset/SSID_Images//78.jpg', 'Dataset/SSID_Images//79.jpg', 'Dataset/SSID_Images//80.jpg', 'Dataset/SSID_Images//81.jpg', 'Dataset/SSID_Images//82.jpg', 'Dataset/SSID_Images//83.jpg', 'Dataset/SSID_Images//84.jpg', 'Dataset/SSID_Images//85.jpg', 'Dataset/SSID_Images//86.jpg', 'Dataset/SSID_Images//87.jpg', 'Dataset/SSID_Images//88.jpg', 'Dataset/SSID_Images//89.jpg', 'Dataset/SSID_Images//90.jpg', 'Dataset/SSID_Images//91.jpg', 'Dataset/SSID_Images//92.jpg', 'Dataset/SSID_Images//93.jpg', 'Dataset/SSID_Images//94.jpg', 'Dataset/SSID_Images//95.jpg', 'Dataset/SSID_Images//96.jpg', 'Dataset/SSID_Images//97.jpg', 'Dataset/SSID_Images//98.jpg', 'Dataset/SSID_Images//99.jpg', 'Dataset/SSID_Images//100.jpg', 'Dataset/SSID_Images//101.jpg', 'Dataset/SSID_Images//102.jpg', 'Dataset/SSID_Images//103.jpg', 'Dataset/SSID_Images//104.jpg', 'Dataset/SSID_Images//105.jpg', 'Dataset/SSID_Images//106.jpg', 'Dataset/SSID_Images//107.jpg', 'Dataset/SSID_Images//108.jpg', 'Dataset/SSID_Images//109.jpg', 'Dataset/SSID_Images//110.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_images = [preprocess_image(image_path) for image_path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "for inputs in processed_images:\n",
    "    outputs = model.generate(**inputs)\n",
    "    caption = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    captions.append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a woman in a green jacket and black pants holding a snowboard', 'a group of people walking down a dirt covered road', 'a skier skiing down a snowy covered mountain', 'a skier skiing down a snowy covered mountain', 'a skier skiing down a snowy covered mountain', 'a skier skiing down a snowy covered mountain', \"the cover of the book, the mountaineers's guide to the ski resort\", 'a skier skiing down a snowy covered mountain', 'a group of people riding ski boards down a snow covered slope', 'a skier on a ski slope with a ski board in his hand', 'a skier skiing down a snowy covered mountain', 'a group of people riding ski boards down a snow covered slope', 'a group of people riding ski boards down a snow covered slope', \"the cover of the album,'the mountain '\", 'a skier makes a turn on a snowy slope in the ski area of the ski resort in whistle', 'a man sitting on a bench in the grass', 'a woman walking through a field of flowers', 'a woman sitting at a table with a whiteboard in front of her', 'a man standing in front of a painting on a wall', 'a room with a desk, chair and book shelf', 'a room filled with lots of cl cl cl cl cl cl cl cl cl cl cl cl cl', 'a man walking down a green hill with a sky background', 'a man walking on a rocky mountain covered in snow', 'a man running through a rocky area with a cloudy sky', 'a man riding a bike down a rocky slope', 'a group of people sitting on top of a rock', 'a large tree in the middle of a forest', 'a person riding a snowboard down a snowy covered slope', 'a man climbing a rock wall with a rope', 'a man climbing a rock wall with a rope', 'a group of people standing around a table with a guitar', 'a man and woman in a car with a baby', 'a man is standing in the back of a truck', 'two men sitting on top of a car in a parking', 'a group of people on a boat in the water', 'a boat sailing through the ocean with a person on it', 'a man surfing on a large wave in the ocean', 'a man in a boat with a hat on', 'a man in a blue shirt and sunglasses holds a surfboard while standing on a beach', 'a surfer rides a large wave in the ocean', 'a man and a woman on a boat in the ocean', 'a man in a wet suit diving under water', 'a book cover with a diver swimming in the ocean', 'a man in a wet suit and a woman in a wet suit, both with a scuba suit', 'a man in a wet suit holding a fish', 'a person in a scuba suit and goggles outfit is diving in the ocean', 'a large group of fish swimming in the ocean', 'a large group of fish swimming in the ocean', 'a large group of fish swimming in the ocean', 'a man in a wet suit and mask holding a spear while standing on a reef with a fish', 'a woman riding a surfboard on top of a wave', 'a person is working on a piece of fish', 'a person holding a grill with a fish on it', 'a man is cutting a piece of meat on a wooden table', 'a group of people standing around a table with food', 'a man is working in a field with a tractor', 'a group of young men playing a game of golf', 'a man in a plaid shirt and plaid pants playing golf', 'a man kneeling on the ground in a forest', 'a man standing on a wooden platform in the woods', 'a bunch of mangos sitting on top of a wooden table', 'a woman walking down a dirt road past a house', 'a man in a hat and blue shirt talking to another man in a blue shirt', 'the crew of the tv show, the big bang bang bang, working on a piece of furniture', 'a man in a red shirt and black pants is standing on a green field', 'a group of people walking through a forest filled with trees', 'a bird flying over a lush green forest', 'the cast of supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural', 'a mountain range with a forest covered in fog', 'a man in a forest with a backpack', 'a view of the mountains and water from a high altitude observation platform', 'a group of people in a body of water', 'a man in a canoe on a river', 'a man and woman stand on a paddle board in a lake', 'a man holding a stick in a river', 'a man in a blue shirt and black pants is talking to another man in a blue shirt', 'a man in a plaid shirt and jeans sitting on a bench', 'a man in a blue shirt and a woman in a red dress', 'a man in a plaid shirt and jeans sitting on a bench', 'a man in a blue shirt and black pants', 'a man standing on top of a rocky beach next to a body of water', 'a large body of water surrounded by rocks', 'a man in a gray shirt and black pants is walking through rubble', 'a man in a hat and shirt holding a guitar', 'a man standing on top of a pile of rubble', 'a boat with people on it floating in the water', 'a man in a blue shirt and black pants is swimming in the ocean', 'a boat in the ocean with a person in it', 'a black and white photo of a person riding a bike', 'a pile of garbage in a yard with a person walking on the ground', 'a man sitting on top of a tree with a guitar', 'a man sitting on top of a tree covered in leaves', 'a view of the ocean from a cliff', 'a man in a boat on the water', 'a beach covered in rocks and pebbles', 'a large body of water surrounded by rocks and trees', 'a man standing on top of a pile of rocks', 'a man is standing on a rocky beach', 'a man standing on top of a pile of trash', 'a fire in the middle of a forest']\n"
     ]
    }
   ],
   "source": [
    "print(captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption for image 11: a woman in a green jacket and black pants holding a snowboard\n",
      "Caption for image 12: a group of people walking down a dirt covered road\n",
      "Caption for image 13: a skier skiing down a snowy covered mountain\n",
      "Caption for image 14: a skier skiing down a snowy covered mountain\n",
      "Caption for image 15: a skier skiing down a snowy covered mountain\n",
      "Caption for image 16: a skier skiing down a snowy covered mountain\n",
      "Caption for image 17: the cover of the book, the mountaineers's guide to the ski resort\n",
      "Caption for image 18: a skier skiing down a snowy covered mountain\n",
      "Caption for image 19: a group of people riding ski boards down a snow covered slope\n",
      "Caption for image 20: a skier on a ski slope with a ski board in his hand\n",
      "Caption for image 21: a skier skiing down a snowy covered mountain\n",
      "Caption for image 22: a group of people riding ski boards down a snow covered slope\n",
      "Caption for image 23: a group of people riding ski boards down a snow covered slope\n",
      "Caption for image 24: the cover of the album,'the mountain '\n",
      "Caption for image 25: a skier makes a turn on a snowy slope in the ski area of the ski resort in whistle\n",
      "Caption for image 26: a man sitting on a bench in the grass\n",
      "Caption for image 27: a woman walking through a field of flowers\n",
      "Caption for image 28: a woman sitting at a table with a whiteboard in front of her\n",
      "Caption for image 29: a man standing in front of a painting on a wall\n",
      "Caption for image 30: a room with a desk, chair and book shelf\n",
      "Caption for image 31: a room filled with lots of cl cl cl cl cl cl cl cl cl cl cl cl cl\n",
      "Caption for image 32: a man walking down a green hill with a sky background\n",
      "Caption for image 33: a man walking on a rocky mountain covered in snow\n",
      "Caption for image 34: a man running through a rocky area with a cloudy sky\n",
      "Caption for image 35: a man riding a bike down a rocky slope\n",
      "Caption for image 36: a group of people sitting on top of a rock\n",
      "Caption for image 37: a large tree in the middle of a forest\n",
      "Caption for image 38: a person riding a snowboard down a snowy covered slope\n",
      "Caption for image 39: a man climbing a rock wall with a rope\n",
      "Caption for image 40: a man climbing a rock wall with a rope\n",
      "Caption for image 41: a group of people standing around a table with a guitar\n",
      "Caption for image 42: a man and woman in a car with a baby\n",
      "Caption for image 43: a man is standing in the back of a truck\n",
      "Caption for image 44: two men sitting on top of a car in a parking\n",
      "Caption for image 45: a group of people on a boat in the water\n",
      "Caption for image 46: a boat sailing through the ocean with a person on it\n",
      "Caption for image 47: a man surfing on a large wave in the ocean\n",
      "Caption for image 48: a man in a boat with a hat on\n",
      "Caption for image 49: a man in a blue shirt and sunglasses holds a surfboard while standing on a beach\n",
      "Caption for image 50: a surfer rides a large wave in the ocean\n",
      "Caption for image 51: a man and a woman on a boat in the ocean\n",
      "Caption for image 52: a man in a wet suit diving under water\n",
      "Caption for image 53: a book cover with a diver swimming in the ocean\n",
      "Caption for image 54: a man in a wet suit and a woman in a wet suit, both with a scuba suit\n",
      "Caption for image 55: a man in a wet suit holding a fish\n",
      "Caption for image 56: a person in a scuba suit and goggles outfit is diving in the ocean\n",
      "Caption for image 57: a large group of fish swimming in the ocean\n",
      "Caption for image 58: a large group of fish swimming in the ocean\n",
      "Caption for image 59: a large group of fish swimming in the ocean\n",
      "Caption for image 60: a man in a wet suit and mask holding a spear while standing on a reef with a fish\n",
      "Caption for image 61: a woman riding a surfboard on top of a wave\n",
      "Caption for image 62: a person is working on a piece of fish\n",
      "Caption for image 63: a person holding a grill with a fish on it\n",
      "Caption for image 64: a man is cutting a piece of meat on a wooden table\n",
      "Caption for image 65: a group of people standing around a table with food\n",
      "Caption for image 66: a man is working in a field with a tractor\n",
      "Caption for image 67: a group of young men playing a game of golf\n",
      "Caption for image 68: a man in a plaid shirt and plaid pants playing golf\n",
      "Caption for image 69: a man kneeling on the ground in a forest\n",
      "Caption for image 70: a man standing on a wooden platform in the woods\n",
      "Caption for image 71: a bunch of mangos sitting on top of a wooden table\n",
      "Caption for image 72: a woman walking down a dirt road past a house\n",
      "Caption for image 73: a man in a hat and blue shirt talking to another man in a blue shirt\n",
      "Caption for image 74: the crew of the tv show, the big bang bang bang, working on a piece of furniture\n",
      "Caption for image 75: a man in a red shirt and black pants is standing on a green field\n",
      "Caption for image 76: a group of people walking through a forest filled with trees\n",
      "Caption for image 77: a bird flying over a lush green forest\n",
      "Caption for image 78: the cast of supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural supernatural\n",
      "Caption for image 79: a mountain range with a forest covered in fog\n",
      "Caption for image 80: a man in a forest with a backpack\n",
      "Caption for image 81: a view of the mountains and water from a high altitude observation platform\n",
      "Caption for image 82: a group of people in a body of water\n",
      "Caption for image 83: a man in a canoe on a river\n",
      "Caption for image 84: a man and woman stand on a paddle board in a lake\n",
      "Caption for image 85: a man holding a stick in a river\n",
      "Caption for image 86: a man in a blue shirt and black pants is talking to another man in a blue shirt\n",
      "Caption for image 87: a man in a plaid shirt and jeans sitting on a bench\n",
      "Caption for image 88: a man in a blue shirt and a woman in a red dress\n",
      "Caption for image 89: a man in a plaid shirt and jeans sitting on a bench\n",
      "Caption for image 90: a man in a blue shirt and black pants\n",
      "Caption for image 91: a man standing on top of a rocky beach next to a body of water\n",
      "Caption for image 92: a large body of water surrounded by rocks\n",
      "Caption for image 93: a man in a gray shirt and black pants is walking through rubble\n",
      "Caption for image 94: a man in a hat and shirt holding a guitar\n",
      "Caption for image 95: a man standing on top of a pile of rubble\n",
      "Caption for image 96: a boat with people on it floating in the water\n",
      "Caption for image 97: a man in a blue shirt and black pants is swimming in the ocean\n",
      "Caption for image 98: a boat in the ocean with a person in it\n",
      "Caption for image 99: a black and white photo of a person riding a bike\n",
      "Caption for image 100: a pile of garbage in a yard with a person walking on the ground\n",
      "Caption for image 101: a man sitting on top of a tree with a guitar\n",
      "Caption for image 102: a man sitting on top of a tree covered in leaves\n",
      "Caption for image 103: a view of the ocean from a cliff\n",
      "Caption for image 104: a man in a boat on the water\n",
      "Caption for image 105: a beach covered in rocks and pebbles\n",
      "Caption for image 106: a large body of water surrounded by rocks and trees\n",
      "Caption for image 107: a man standing on top of a pile of rocks\n",
      "Caption for image 108: a man is standing on a rocky beach\n",
      "Caption for image 109: a man standing on top of a pile of trash\n",
      "Caption for image 110: a fire in the middle of a forest\n"
     ]
    }
   ],
   "source": [
    "for i, caption in enumerate(captions, start=10):\n",
    "    print(f\"Caption for image {i+1}: {caption}\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
